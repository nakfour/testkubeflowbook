<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">@import url('https://themes.googleusercontent.com/fonts/css?kit=cGvuclDC_Z1vE_cnVEU6Ae-HO_0mhOaSyNXUatljuZqz0PeW-eYr9Do1xGSoOMyWHGVhd4QBzDWGUS9DGN8tcw');.lst-kix_list_2-6>li:before{content:"\0025cf  "}.lst-kix_list_2-7>li:before{content:"\0025cb  "}ul.lst-kix_list_1-0{list-style-type:none}.lst-kix_list_2-4>li:before{content:"\0025cb  "}.lst-kix_list_2-5>li:before{content:"\0025a0  "}.lst-kix_list_2-8>li:before{content:"\0025a0  "}.lst-kix_list_3-0>li:before{content:"\0025cf  "}.lst-kix_list_3-1>li:before{content:"\0025cb  "}.lst-kix_list_3-2>li:before{content:"\0025a0  "}ul.lst-kix_list_3-7{list-style-type:none}ul.lst-kix_list_3-8{list-style-type:none}ul.lst-kix_list_1-3{list-style-type:none}ul.lst-kix_list_3-1{list-style-type:none}.lst-kix_list_3-5>li:before{content:"\0025a0  "}ul.lst-kix_list_1-4{list-style-type:none}ul.lst-kix_list_3-2{list-style-type:none}ul.lst-kix_list_1-1{list-style-type:none}.lst-kix_list_3-4>li:before{content:"\0025cb  "}ul.lst-kix_list_1-2{list-style-type:none}ul.lst-kix_list_3-0{list-style-type:none}ul.lst-kix_list_1-7{list-style-type:none}.lst-kix_list_3-3>li:before{content:"\0025cf  "}ul.lst-kix_list_3-5{list-style-type:none}ul.lst-kix_list_1-8{list-style-type:none}ul.lst-kix_list_3-6{list-style-type:none}ul.lst-kix_list_1-5{list-style-type:none}ul.lst-kix_list_3-3{list-style-type:none}ul.lst-kix_list_1-6{list-style-type:none}ul.lst-kix_list_3-4{list-style-type:none}.lst-kix_list_3-8>li:before{content:"\0025a0  "}.lst-kix_list_3-6>li:before{content:"\0025cf  "}.lst-kix_list_3-7>li:before{content:"\0025cb  "}ul.lst-kix_list_2-8{list-style-type:none}ul.lst-kix_list_2-2{list-style-type:none}.lst-kix_list_1-0>li:before{content:"\0025cf  "}ul.lst-kix_list_2-3{list-style-type:none}ul.lst-kix_list_2-0{list-style-type:none}ul.lst-kix_list_2-1{list-style-type:none}ul.lst-kix_list_2-6{list-style-type:none}.lst-kix_list_1-1>li:before{content:"\0025cb  "}.lst-kix_list_1-2>li:before{content:"\0025a0  "}ul.lst-kix_list_2-7{list-style-type:none}ul.lst-kix_list_2-4{list-style-type:none}ul.lst-kix_list_2-5{list-style-type:none}.lst-kix_list_1-3>li:before{content:"\0025cf  "}.lst-kix_list_1-4>li:before{content:"\0025cb  "}.lst-kix_list_1-7>li:before{content:"\0025cb  "}.lst-kix_list_1-5>li:before{content:"\0025a0  "}.lst-kix_list_1-6>li:before{content:"\0025cf  "}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}.lst-kix_list_2-0>li:before{content:"\0025cf  "}.lst-kix_list_2-1>li:before{content:"\0025cb  "}.lst-kix_list_1-8>li:before{content:"\0025a0  "}.lst-kix_list_2-2>li:before{content:"\0025a0  "}.lst-kix_list_2-3>li:before{content:"\0025cf  "}ol{margin:0;padding:0}table td,table th{padding:0}.c38{border-right-style:solid;border-top-width:0pt;border-bottom-color:#000000;border-right-width:0pt;padding-left:0pt;border-left-color:#000000;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-top-color:#000000;border-bottom-style:solid;padding-right:0pt;height:13pt}.c44{border-right-style:solid;border-top-width:0pt;border-bottom-color:#000000;border-right-width:0pt;padding-left:0pt;border-left-color:#000000;border-right-color:#000000;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-top-color:#000000;border-bottom-style:solid;padding-right:0pt}.c35{background-color:#85200c;color:#f3f3f3;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Verdana";font-style:normal}.c40{background-color:#f8f8f8;color:#aa22ff;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Consolas";font-style:normal}.c25{background-color:#f8f8f8;color:#222222;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10.5pt;font-family:"Consolas";font-style:normal}.c5{background-color:#f3f3f3;color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Roboto Mono";font-style:normal}.c31{color:#85200c;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Verdana";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Verdana";font-style:normal}.c37{color:#85200c;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Verdana";font-style:italic}.c22{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Verdana";font-style:italic}.c36{color:#85200c;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:40pt;font-family:"Arial";font-style:italic}.c15{color:#85200c;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:13pt;font-family:"Verdana";font-style:italic}.c51{color:#85200c;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:21pt;font-family:"Verdana";font-style:italic}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:7pt;font-family:"Arial";font-style:normal}.c2{padding-top:5pt;padding-bottom:4pt;line-height:1.1400000000000001;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c12{color:#85200c;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Verdana";font-style:normal}.c45{font-weight:400;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Verdana";font-style:normal}.c49{font-weight:700;text-decoration:none;vertical-align:baseline;font-size:8pt;font-family:"Verdana";font-style:normal}.c30{margin-left:36pt;padding-top:0pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1400000000000001;text-align:left}.c13{margin-left:36pt;padding-top:10pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c8{padding-top:5pt;padding-bottom:5pt;line-height:1.0;orphans:2;widows:2;text-align:right}.c33{margin-left:36pt;padding-top:5pt;padding-left:0pt;padding-bottom:0pt;line-height:1.1400000000000001;text-align:left}.c11{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;orphans:2;widows:2;text-align:left}.c1{padding-top:5pt;padding-bottom:5pt;line-height:1.1400000000000001;text-align:left;height:8pt}.c17{padding-top:0pt;padding-bottom:0pt;line-height:1.1500000000000001;text-align:left}.c53{padding-top:0pt;padding-bottom:10pt;line-height:1.1500000000000001;text-align:left}.c3{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c28{font-size:13.5pt;font-family:"Overpass";color:#151515;font-weight:400}.c41{padding-top:5pt;padding-bottom:5pt;line-height:1.1400000000000001;text-align:center}.c20{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:center}.c21{padding-top:0pt;padding-bottom:8pt;line-height:1.0;text-align:left}.c4{padding-top:5pt;padding-bottom:5pt;line-height:1.0;text-align:left}.c55{padding-top:10pt;padding-bottom:10pt;line-height:1.1500000000000001;text-align:left}.c56{padding-top:14pt;padding-bottom:0pt;line-height:1.1400000000000001;text-align:left}.c52{padding-top:5pt;padding-bottom:100pt;line-height:2.0;text-align:right}.c7{padding-top:5pt;padding-bottom:5pt;line-height:1.1400000000000001;text-align:left}.c39{-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline;text-decoration-skip-ink:none}.c42{padding-top:0pt;padding-bottom:0pt;line-height:1.1400000000000001;text-align:left}.c14{color:#000000;text-decoration:none;vertical-align:baseline;font-style:normal}.c50{padding-top:0pt;padding-bottom:5pt;line-height:1.1400000000000001;text-align:left}.c16{font-size:7pt;font-family:"Arial";font-weight:400}.c6{font-size:7pt;font-family:"Arial";font-weight:700}.c19{font-weight:400;font-size:11pt;font-family:"Arial"}.c27{max-width:378pt;padding:72pt 72pt 72pt 72pt}.c47{margin-left:36pt;padding-left:0pt}.c43{padding:0;margin:0}.c26{color:inherit;text-decoration:inherit}.c23{border:1px solid black;margin:5px}.c29{width:33%;height:1px}.c46{orphans:2;widows:2}.c48{height:10pt}.c32{font-size:10pt}.c54{color:#151515}.c34{vertical-align:super}.c24{color:#ff0000}.c10{background-color:#ffffff}.c18{height:8pt}.title{padding-top:5pt;color:#85200c;font-weight:700;font-size:48pt;padding-bottom:5pt;font-family:"Arial";line-height:1.0;font-style:italic;text-align:right}.subtitle{padding-top:5pt;color:#85200c;font-size:21pt;padding-bottom:100pt;font-family:"Verdana";line-height:2.0;font-style:italic;text-align:right}li{color:#000000;font-size:8pt;font-family:"Verdana"}p{margin:0;color:#000000;font-size:8pt;font-family:"Verdana"}h1{border-right-style:solid;padding-top:0pt;color:#85200c;border-top-width:0pt;border-bottom-color:#000000;font-weight:700;border-right-width:0pt;padding-left:0pt;border-left-color:#000000;font-size:16pt;padding-bottom:0pt;line-height:1.1400000000000001;border-right-color:#000000;font-style:italic;border-left-width:0pt;border-top-style:solid;background-color:#ffffff;border-left-style:solid;border-bottom-width:0pt;border-top-color:#000000;font-family:"Verdana";border-bottom-style:solid;text-align:left;padding-right:0pt}h2{border-right-style:solid;padding-top:0pt;color:#85200c;border-top-width:0pt;border-bottom-color:#000000;font-weight:700;border-right-width:0pt;padding-left:0pt;border-left-color:#000000;font-size:13pt;padding-bottom:0pt;line-height:1.1400000000000001;border-right-color:#000000;font-style:italic;border-left-width:0pt;border-top-style:solid;border-left-style:solid;border-bottom-width:0pt;border-top-color:#000000;font-family:"Verdana";border-bottom-style:solid;text-align:left;padding-right:0pt}h3{padding-top:16pt;color:#85200c;font-weight:700;font-size:10pt;padding-bottom:4pt;font-family:"Verdana";line-height:1.1400000000000001;text-align:left}h4{background-color:#85200c;padding-top:14pt;color:#f3f3f3;font-weight:700;font-size:9pt;padding-bottom:0pt;font-family:"Verdana";line-height:1.1400000000000001;text-align:left}h5{padding-top:11pt;color:#980000;font-weight:700;font-size:10pt;padding-bottom:5pt;font-family:"Verdana";line-height:1.1400000000000001;text-align:left}h6{padding-top:5pt;color:#85200c;font-weight:700;font-size:8pt;padding-bottom:4pt;font-family:"Verdana";line-height:1.1400000000000001;text-align:left}</style></head><body class="c10 c27 doc-content"><p class="c8 title" id="h.gjdgxs"><span>&nbsp;</span><span>1 Data Science at Scale </span><sup><a href="#cmnt1" id="cmnt_ref1">[a]</a></sup></p><p class="c46 c18 c52 subtitle" id="h.30j0zll"><span class="c51"></span></p><p class="c55"><span class="c22">This chapter covers</span></p><ul class="c43 lst-kix_list_1-0 start"><li class="c13 li-bullet-0"><span class="c0">Understanding the steps involved in developing data-centric applications and describing how Kubeflow helps in managing data-centric workflows.</span></li><li class="c47 c53 li-bullet-0"><span>Understanding containers and how Kubernetes helps manage multiple running containers (also known as container </span><span>deployments</span><span class="c0">).</span></li></ul><p class="c7"><span class="c0">The rise of internet companies within the last two decades has led to a glut of data. Large-scale data collection has been enabled by advances in high-speed networking and data storage technologies. This has led to the crystallization of a new field called data science. While most aspects of data science have existed for decades and while very sophisticated data analysis techniques have been used in the past, the pervasiveness of these techniques and technologies is unprecedented. </span></p><p class="c7"><span class="c0">Given the breadth and depth of all the skills required to effectively use data, it&rsquo;s very unlikely, if not impossible, for an individual to have expertise in more than a few of the core skills. Tools that let one focus on understanding the problem, the data and building effective statistical models and abstract away infrastructure concerns act as force-multipliers for data scientists. </span></p><p class="c7"><span>Kubeflow is a </span><span>framework that provides a </span><span>collection of tools that spans the full extent of a data scientist&rsquo;s workflow, from data preprocessing through model training to deployment in production. This framework is built on top of Kubernetes, a system for coordinating and controlling the deployment of artifacts called containers that are described in more detail</span><span>&nbsp;</span><span>in this chapter</span><span class="c0">. Our goal is to make you fluent with Kubeflow and its components so that you can take a project from prototype to production as rapidly as possible.</span></p><p class="c1"><span class="c0"></span></p><h1 class="c44 c11" id="h.1fob9te"><span>1.1 Why are Kubeflow and Kubernetes </span><span>needed</span><sup><a href="#cmnt2" id="cmnt_ref2">[b]</a></sup><span class="c37 c10">?</span></h1><p class="c7"><span class="c0">The first question one might ask is if any of these tools are necessary? After all, one could simply install Python and the relevant packages through pip (or an equivalent package manager) on your laptop&rsquo;s operating system and get started with real data science work within minutes. While this would certainly work for quick prototyping and even for small research projects, one quickly encounters problems when doing this at an industrial scale where performance, reliability and reproducibility are of paramount importance.</span></p><p class="c7"><span class="c0">To understand the gap filled by Kubeflow, we need to take a look at the typical workflow of a data scientist in figure 1.1.</span></p><p class="c41"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 356.34px; height: 452.50px;"><img alt="" src="images/image1.png" style="width: 356.34px; height: 452.50px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.3znysh7"><span class="c12">Figure 1.1 An Idealized Data Science Workflow. Real-life projects involve multiple iterations of these steps. For example, if the modeling shows unsatisfactory results then one might collect more data and repeat the modeling step. Kubeflow provides tools to implement these steps for a data science (DS) / machine learning (ML) / artificial intelligence (AI) workflow.</span></h6><p class="c7"><span class="c0">One disclaimer is that these steps are not necessarily always executed in the strict order shown in the figure and often, there are multiple iterations of some steps:</span></p><ul class="c43 lst-kix_list_3-0 start"><li class="c33 li-bullet-0"><span>Data collection and storage: For most projects, some data already exists although this data might be incomplete for the task at hand.<br></span></li><li class="c30 li-bullet-0"><span>Data exploration and cleaning: These two steps are heavily intertwined. Exploration involves asking a series of questions and answering them with the available data. This exposes any flaws with the data collection process and often leads to either gathering more data, using the existing dataset to remedy the flaws &nbsp;(&ldquo;cleaning&rdquo;) or to even changing the problem one is trying to solve.<br></span></li><li class="c30 li-bullet-0"><span>Modeling: Many problems don&rsquo;t reach this stage and that&rsquo;s a good thing. A lot of practical problems are often solved by simple queries and exploratory plots. Sometimes though, the goal is to either predict future behavior, or the behavior of entities not encountered yet in the data, or to understand more complex patterns. All of these require building statistical models. In general, models don&rsquo;t have to be machine learning models which are very popular today and cover everything from linear regression to tree-based models to deep neural networks. An alternate example is a first-principles based model consisting of partial differential equations describing a physical system with some free parameters that are inferred from the dataset. Modeling requires setting up the data to be model-ready, choosing a class of models and then running the training or learning procedure. Once the model is trained, it is evaluated and often improved by either changing the data (picking new features) or tweaking the model itself (new class of models, hyperparameter tuning, different network architectures for neural networks etc.).<br><br>From a practical viewpoint, this step often involves running multiple experiments with variations in the dataset, modeling techniques, as well as model parameters. These experiments are highly parallelizable and having tools to scale and automate both the execution of the training step as well as the tracking and monitoring of the experiments is crucial at this stage.<br></span></li><li class="c7 c47 li-bullet-0"><span>Deployment: Whether one is working on reports based on exploratory data analysis or trained models making predictions on new data, the goal is to have code that processes the data running in production. This involves many aspects including monitoring and logging as well as tools to scale processing as the amount of data increases. In our experience, most data science projects don&rsquo;t make it to the deployment stage. Sometimes this is desirable since data science projects, by their very nature, are often exploratory. Physics graduate students are often told to &ldquo;get a good waste-paper basket&rdquo; since the majority of one&rsquo;s work leads to dead-ends and the same applies to data scientists. On the other hand, there are many data science projects that are promising but don&rsquo;t end up being deployed because of inadequate infrastructure. <br><br>In the case that a model has clear commercial value, it is obvious that it should be deployed. But deployment is also useful to test how a model would work on new, incoming data and get real-time feedback that can lead to improvements and tweaks. Lastly, as a psychological measure, just as it&rsquo;s very satisfying for a software engineer to ship a product, it&rsquo;s very satisfying for a data scientist to see their model in production.</span></li></ul><p class="c7"><span>Kubeflow provides tools to structure all the above tasks. These tools include Jupyter notebooks for data exploration, cleaning and modeling, Katib for hyperparameter tuning, Pipelines to structure complex data transformations that are used to create model-ready data, and K</span><span>FServing</span><span class="c0">&nbsp;for scalable model deployment. This is not an exhaustive list and new tools and features are constantly being developed.</span></p><p class="c7"><span class="c0">A central feature of Kubeflow is the use of containers which can be thought of as self-contained artifacts or entities that contain the code you want to run and the dependencies or libraries the code needs. Kubeflow is built on top of a framework called Kubernetes which is a system for managing containers. This makes it easy to run multiple workloads, each in its own container with its own set of libraries and library versions on the same physical hardware. It also makes it easy to scale workloads just by deploying more containers running the same applications.</span></p><p class="c1"><span class="c0"></span></p><h1 class="c11 c44" id="h.2et92p0"><span class="c37 c10">1.2 Kubeflow and Open Data Hub</span></h1><p class="c7"><span class="c0">Kubeflow started as an open source project from Google and was based on an internal project that ran TensorFlow jobs on Kubernetes. The initial release of Kubeflow was in March 2018 and it included only Jupyter for launching Notebooks, TensorFlow training and serving. Since then, Kubeflow has evolved into a comprehensive end-to-end machine learning platform running on Kubernetes. The community has also grown immensely with regular active participation from many different companies. However, since it is an open source project, documentation is always lacking or lagging behind the latest releases which makes it difficult for users to learn and use Kubeflow. It is important to note that Kubeflow is built by software engineers and the documentation always assumes some level of understanding that Data Scientists or Data Engineers most often do not possess. The community constantly strives to gather feedback from Data Scientists and Engineers that are the main users of this platform and uses this information to adapt the roadmap and the user experience. </span></p><p class="c7"><span class="c0">Today Kubeflow includes many different components that can be used in different phases of the AI/ML workflow. Kubeflow includes the following tools:</span></p><ul class="c43 lst-kix_list_2-0 start"><li class="c33 li-bullet-0"><span>Development environment tools for Data Scientists and Data Engineers such as Jupyter Notebooks.</span></li><li class="c30 li-bullet-0"><span>Tools for model development such as distributed training, validation and serving. &nbsp;</span></li><li class="c30 li-bullet-0"><span>Data ETL (Extract, Transform and Load) tools and metadata that can be used for data preparation and version control. </span></li><li class="c47 c50 li-bullet-0"><span>Pipeline tools such as Kubeflow pipelines for automation and production ready machine learning workflows.</span></li></ul><p class="c7"><span>Kubeflow is constantly evolving and many new features are constantly being added. You will also notice that there are many gaps in the tools and user experience provided by Kubeflow particularly in the area of </span><span>multi-user authorization and authentication,</span><span>&nbsp;data governance, full scale reproducibility or lineage in both experimentation and production workflows, and monitoring. </span><span>We will describe these areas in more detail in later chapters.</span><sup><a href="#cmnt3" id="cmnt_ref3">[c]</a></sup></p><p class="c7"><span>What makes Kubeflow powerful is the ability to run Kubeflow on Kubernetes running on any cloud provider such as AWS, Azure</span><span class="c0">&nbsp;and IBM. It provides a seamless platform for engineers to run AI/ML workflows on Kubernetes with many of the Kubernetes specifics abstracted from the user. Since it is running on Kubernetes, scaling functionality for all microservices is provided and supported in Kubeflow. </span></p><p class="c7"><span class="c0">Installation of Kubeflow on Kubernetes can be as simple as a couple of UI clicks as provided by the operators we describe later or a bit more involved and requires downloading specific tools and editing yaml files. We will describe in detail how to install Kubeflow in the next chapter.</span></p><p class="c7"><span>Open Data Hub is also an open source project that provides an end</span><span>&nbsp;</span><span>to</span><span>&nbsp;</span><span>end AI/ML platform on OKD. Open Data Hub includes Kubeflow and many other open source AI/ML tools in one operator to be installed on OKD. Open Data Hub is currently available as a community operator in the OKD OperatorHub catalog. Open Data Hub started as an internal project </span><span>in</span><span>&nbsp;Red Hat providing object storage and JupyterHub notebooks to internal Data Scientists and Data Engineers to perform their AI/ML tasks. The project grew since releasing it as an open source project and now includes many different open source components as shown in the image below. You will notice that many of these components are not included in Kubeflow such as Kafka messaging, Grafana and Prometheus for monitoring. We will describe in detail all the components included in the Open Data Hub project in </span><span>later chapters</span><sup><a href="#cmnt4" id="cmnt_ref4">[d]</a></sup><span>. These components are shown in figure 1.2.</span><sup><a href="#cmnt5" id="cmnt_ref5">[e]</a></sup></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 540.00px;"><img alt="" src="images/image3.png" style="width: 504.00px; height: 540.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.tyjcwt"><span class="c12">Figure 1.2 Open Data Hub in OperatorHub on OKD. Open Data Hub provides an end-to-end AI/ML platform that involves not just Kubeflow but many other tools like Kafka, Prometheus and Spark, as well.</span></h6><p class="c1"><span class="c0"></span></p><p class="c7"><span>To fully understand and be able to use Kubeflow, it is imperative to understand many basic concepts and components of the platform it runs on. In the following sections we will describe at a high level many different components, starting with a description of the evolution of backend cloud computing systems and relevant components of Kubernetes</span><span>(kubernetes.io) and OKD</span><span class="c0">(okd.io). This book attempts to just guide you through these concepts and does not include a deep dive into them. We believe that having a good understanding of these concepts will allow users to better understand Kubeflow and its capabilities as an AI/ML platform running on Kubernetes and OKD. </span></p><h2 class="c44 c11" id="h.3dy6vkm"><span class="c15">1.2.1 Operating Systems, Virtual Machines and Containers </span></h2><p class="c7"><span class="c0">Let&rsquo;s take a step back to understand the fundamentals of computing hardware and software as a platform to run AI/ML workloads. Most data scientists or even software developers know only the basics which is just enough to run their applications or workloads. And you might be asking, Well why do I need to know the internals? The answer is simple. When you are in a discussion with devops engineers about virtual machines, containers, memory, processing power of the platform running your AI/ML workload, your understanding of the terms and ability to contribute to solving these issues will be immensely improved and efficient if you have a deeper understanding of the platform. </span></p><p class="c7"><span>Starting with operating systems which is the actual process (or multiple processes/threads) that runs the computing machine. The operating system is responsible for managing all components such as memory, network interfaces, disk space and processors. &nbsp;There are many different operating systems managing many different computing machines. If you want to understand the cloud computing environment, it is vital to understand the operating systems running these platforms. Linux is the most popular operating system running on cloud computing environments, </span><span class="c10">According to the Linux Foundation</span><sup class="c10 c34"><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c10">, </span><span class="c10"><a class="c26" href="https://www.google.com/url?q=https://www.linuxfoundation.org/2017-linux-kernel-report-landing-page/&amp;sa=D&amp;source=editors&amp;ust=1677450530341442&amp;usg=AOvVaw1zJrav8RZtCPLPnI98AfXb">nine of the top ten public clouds run on Linux</a></span><span class="c0">&nbsp;comprising 90% of all cloud computing environments . It is also the most popular operating system in containers, a core component of Kubernetes that will be described later. For these reasons we will focus on describing the Linux operating system. &nbsp;In simplest terms we can describe Linux as an operating system with two memory spaces: Kernel Space, and User Space, as shown in figure 1.3.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 248.00px;"><img alt="" src="images/image2.png" style="width: 504.00px; height: 248.00px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.1t3h5sf"><span class="c12">Figure 1.3 Linux Operating System. The task of an operating system is to abstract away the specifics of the hardware (CPU, memory, storage, networking) and provide a way to manage resources for applications. Linux separates processes, which are the basic units of computation, into a user space with restricted privileges and a kernel space with full privileges.</span></h6><p class="c7"><span class="c0">The kernel space is a protected space and includes all the processes that drive and run the hardware such as memory, disk space, network interfaces and processors. The user space is where user applications run. The Linux operating system does provide system API calls to the kernel and those are protected too. &nbsp;There are many different users of a Linux operating system with different permissions. The &ldquo;root&rdquo; user is the highest level and has the most permissions and hence the most dangerous. It is important to understand these concepts because when you run your AI/ML workloads in containers you will face permission limitations set forth by the platform administrators to protect the system (e.g., everyone knows &ldquo;root&rdquo; is dangerous). </span></p><p class="c7"><span>What are virtual machines (VMs) and why do we need them? You can think of virtual machines as machines inside of other machines. For example, if I own a Linux machine and want to run Windows, I have to use a virtual machine that runs the Windows Operating system in my Linux machine. In this case the Linux machine is called the &ldquo;host&rdquo; machine and the Windows machine is sometimes called &ldquo;guest&rdquo;. In order to accomplish this there is a tool called a hypervisor that manages these virtual machines. A hypervisor manages and provides a virtual view of the host&rsquo;s machine resources such as memory, processing, disk space and networking. Linux come with a hypervisor called </span><span class="c10 c54">Kernel-based Virtual Machine (KVM)</span><span class="c10 c28">.</span><span>&nbsp; These components are displayed in figure 1.4.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 373.33px;"><img alt="" src="images/image5.png" style="width: 504.00px; height: 373.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.4d34og8"><span>Figure 1.4 Virtual Machines: The first successful step in isolating multiple applications and their dependencies/libraries from each other on the same hardware. Virtual Machines (VMs) enable the running of multiple operating systems (OS) on the same hardware. Each VM can have its own custom environment without interfering with other VMs running on the same hardware.</span></h6><p class="c1"><span class="c0"></span></p><p class="c7"><span class="c0">Applications running within the virtual machine run as if they are in a normal operating system, and resources are available to them in the same manner. However they run in an isolated environment and are unaware of what is happening in the host machine. Isolation is a double edged sword that enables security but complicates networking which we will discuss later.</span></p><p class="c7"><span class="c0">So why do we need virtual machines and why did we complicate the system? The answer is to provide the following: scalability, isolation, efficient resource management and high resource utilization. In simple terms the hypervisor is able to efficiently manage the resources between different machines allowing for more robust scalability. Isolation of applications is very powerful from a resource management and security perspective. If you think of it, with the introduction of virtual machines there are two levels of security or permission gatekeepers, there is a virtual machine and the host machine. If one application within a Virtual Machine fails or breaches security, it is isolated from the host machine. There are some disadvantages as well. Running multiple copies of operating systems uses valuable resources that can now not be used for running application software. </span></p><p class="c7"><span>And here come containers to reduce some of this footprint. Containers include your application along with all necessary files to run the application. To create the container image you need to create a script that specifies what goes in the image, usually called a Dockerfile. The script needs to contain all the necessary files and libraries that are essential to run your application in addition to your application. At a maximum it can include all the files necessary to install a Linux distribution and at a minimum it can include none. To help you understand we include two examples below. Example 1 shows a minimal script &nbsp;and example 2 shows a Linux distribution called CentOS. </span></p><p class="c7"><span>Looking at example 1, which is taken from Dockerhub</span><span class="c24">&nbsp;</span><span>(</span><span><a class="c26" href="https://www.google.com/url?q=https://shortener.manning.com/zQqa&amp;sa=D&amp;source=editors&amp;ust=1677450530343365&amp;usg=AOvVaw3CjtSxZj3e6M_wEcETS8Yw">http://mng.bz/zQqa</a></span><span class="c0">), you will notice the first line indicates to include scratch. Scratch is empty and is provided by Docker for simple containers that don&rsquo;t need any files such as &ldquo;hello world&rdquo; examples. The second line tells Docker to copy the &ldquo;hello&rdquo; binary to the folder &ldquo;/&rdquo; inside the container. The third tells Docker to run the binary &ldquo;/hello&rdquo;. This is a very simple &ldquo;Hello World&rdquo; example that shows creating a simple container. </span></p><p class="c4"><span class="c9">FROM scratch</span></p><p class="c4"><span class="c9">COPY hello /</span></p><p class="c4"><span class="c9">CMD [&quot;/hello&quot;]</span></p><p class="c4"><span class="c0">In the second example we show a Dockerfile that includes a Linux distribution called Centos version 7. </span></p><p class="c3"><span class="c9">FROM centos:7</span></p><p class="c3"><span class="c9">RUN yum -y install epel-release</span></p><p class="c3"><span class="c9">RUN yum -y install python-pip &amp;&amp; yum clean all</span></p><p class="c3"><span class="c9">COPY . /pythonapp</span></p><p class="c3"><span class="c9">RUN pip install -r /pythonapp/requirements.txt</span></p><p class="c3"><span class="c9">CMD python /pythonapp/app.py</span></p><p class="c18 c21"><span class="c9"></span></p><p class="c4"><span>In this example we start with including all the base files from centOS version 7. We also install the python &ldquo;pip&rdquo; tool so we can use it to install libraries in the &ldquo;requirements.txt&rdquo; file that is needed by &ldquo;app.py&rdquo; Then we copy in our code that is in the same directory as this script or Dockerfile. We run &ldquo;pip install&rdquo; to install all dependencies for our python app. Then we specify the command to run when running this container, which is basically running our application &ldquo;app.py&rdquo;. Writing Dockerfiles can get complicated and there are many different commands you can include. Please refer to the Dockerfile api </span><span><a class="c26" href="https://www.google.com/url?q=https://shortener.manning.com/GGMq&amp;sa=D&amp;source=editors&amp;ust=1677450530344761&amp;usg=AOvVaw0A-NQh1mNYPkZWO-SKpg2B">http://mng.bz/GGMq</a></span><span class="c0">&nbsp;for a comprehensive description. A deep dive into writing Dockerfiles is out of scope for this book. It is imperative to know that you will most certainly need to learn how to write Dockerfiles for any application that needs to run in a Kubernetes cluster. There are some tools that abstract this process, such as the &ldquo;s2i&rdquo; tool from OKD which we will discuss later.</span></p><p class="c4"><span class="c0">At this point, we have described a script or Dockerfile that creates the container, but who reads it and how does it run? Looking at &nbsp;figure 1.5, you can see that the hypervisor is replaced by the container runtime. The container runtime reads the script, creates the image, manages the container and its images. There are many different container runtimes. The original and most popular one is Docker. On OKD we use CRI-O. </span></p><p class="c4 c18"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 362.67px;"><img alt="" src="images/image4.png" style="width: 504.00px; height: 362.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.2s8eyo1"><span class="c12">Figure 1.5 Container Deployment. Containers are another successful attempt at isolating workloads and their dependencies/libraries. Instead of running a full operating system kernel for each workload (like VMs), there is only one OS kernel running on the system and the separation between workloads is at a much higher level in the stack i.e. at the user-space level. </span></h6><p class="c4"><span class="c0">Another term you might have heard mentioned with Docker, is registry. What is a registry and where is it located? You can think of a registry as a place to store container images. Why do we need to store the images? Simply, because you don&rsquo;t want to build images every time you run your container. Typically you build the image on code changes, and push it to the registry. When the container needs to run, the container runtime tool downloads the image. &nbsp;</span></p><p class="c4"><span class="c0">But where is this &ldquo;registry&rdquo;? Registries are typically hosted on the cloud and are accessible to the cluster. Registries can be public such as Dockerhub and Quay.io or private and require access keys to upload and download images. It is important to understand that your images will be stored in two places. The first place is the remote registry after you push your container image. The second place is locally on the platform the container runtime is running on.</span></p><p class="c4"><span class="c0">The container runtime keeps a local cache of all images downloaded so far for efficiency and better network utilization. However sometimes this causes issues with cache space, and you can find yourself running out of cache space after running many different containers. Most container runtimes offer tools to clean up and maintain the cache. For example, in Docker you can prune or clean unused images by using this command</span></p><p class="c7"><span class="c16">docker image prune</span></p><p class="c1"><span class="c0"></span></p><h2 class="c44 c11" id="h.17dp8vu"><span class="c15">1.2.2 Kubernetes and OKD </span></h2><p class="c7"><span class="c0">Now that we understand containers and container runtimes, we can dive into the Kubernetes world. In simple terms, Kubernetes is a container-orchestration system, meaning it manages and runs the containers we just described. However, Kubernetes does a lot more than that, it actually orchestrates and manages almost every aspect of the cloud such as networking, memory and processing. You can think of it as an operating system managing the cloud that runs on top of the operating systems we discussed earlier, such as Linux. However a huge add on is the fact Kubernetes manages a system that spans multiple machines that form the cloud or cluster. You can think of a cluster as the system with multiple machines managed by Kubernetes. </span></p><p class="c7"><span class="c0">Kubernetes is an open source project that was started by Google and was based on an internal project called borg that Google used to manage and run clusters in their backend cloud. The first release of Kubernetes v1.0 was in July 2015 under the Cloud Native Computing Foundation which is part of the Linux Foundation. Many different companies contributed and grew Kubernetes over the years making it the most popular container management platform for backend clouds. </span></p><p class="c7"><span class="c0">You might have heard the two words Kubernetes and OKD interchanged many times and were wondering what the difference between the two is? Let&rsquo;s clarify some terms. The community open source project for Openshift used to be called &ldquo;Openshift Origin&rdquo; till 2018 when it was renamed to &ldquo;OKD&rdquo; (okd.io). OKD is an open source project available for anyone to deploy and use. Red Hat also provides an Openshift suite of products under the name &ldquo;Red Hat Openshift&rdquo; (openshift.com) that are based on OKD.. </span></p><p class="c7"><span>OKD takes Kubernetes and adds, sometimes changes, components to make it enterprise ready. But how does that have an impact on your work as a data scientist or a data engineer? OKD for example tightens security on containers to eliminate security threats. As an example, in the following listing, we show the Dockerfile from Katib a component of Kubeflow that originally did not specify a UserId. You might create a container from the Dockerfile below for Katib and be able to run it on Kubernetes. However, when you try running it on OKD, it fails to run. If you do not specify the USERID it will inherit the parent USERID from line #2 which was most likely a &ldquo;root&rdquo; user. OKD blocks any container asking for root permissions since as we described earlier it is dangerous. One of our team members actually submitted the patch to add &ldquo;USERID 1000&rdquo; (</span><span><a class="c26" href="https://www.google.com/url?q=https://shortener.manning.com/KBpK&amp;sa=D&amp;source=editors&amp;ust=1677450530346684&amp;usg=AOvVaw1zcqS8p-zOxJxN6cZCRnqK">http://mng.bz/KBpK</a></span><span class="c0">).</span></p><h4 class="c46 c56" id="h.3rdcrjn"><span class="c35">Listing 1.1 </span></h4><p class="c3"><span class="c5">FROM golang:alpine AS build-env #A</span></p><p class="c3 c18"><span class="c5"></span></p><p class="c3"><span class="c5">ADD ./go/src/github.com/kubeflow/katib #B</span></p><p class="c3 c18"><span class="c5"></span></p><p class="c3"><span class="c5">WORKDIR /go/src/github.com/kubeflow/katib/cmd/katib-controller</span></p><p class="c3 c18"><span class="c5"></span></p><p class="c3"><span class="c5">RUN if [ &ldquo;$(uname -m)&rdquo; = &ldquo;ppc64le&rdquo; ]; then \ #C</span></p><p class="c3"><span class="c5">&nbsp; &nbsp; CGO_ENABLED=0 GOOS=linux GOARCH=ppc64le go build -a -o katib-controller ./y1alpha3; \ #C</span></p><p class="c3"><span class="c5">&nbsp; elif [ &ldquo;$(uname -m)&rdquo; = &ldquo;aarch64&rdquo; ]; then \ #C</span></p><p class="c3"><span class="c5">&nbsp; &nbsp; CGO_ENABLED=0 GOOS=linux GOARCH=arm64 go build -a -o katib-controller ./y1alpha3; \ #C</span></p><p class="c3"><span class="c5">&nbsp; else \ #C</span></p><p class="c3"><span class="c5">&nbsp; &nbsp; CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o katib-controller ./y1alpha3; \ #C</span></p><p class="c3"><span class="c5">&nbsp; fi #C</span></p><p class="c3 c18"><span class="c5"></span></p><p class="c3"><span class="c5">FROM alpine:3.7</span></p><p class="c3"><span class="c5">WORKDIR /app</span></p><p class="c3"><span class="c5">RUN apk update &amp;&amp; apk add ca-certificates</span></p><p class="c3"><span class="c5">COPY --from=build-env /go/src/github.com/kubeflow/katib/cmd/katib-controller/katib-controller . #D</span></p><p class="c3"><span class="c5">ENTRYPOINT [&ldquo;./katib-controller&rdquo;]</span></p><p class="c7"><span class="c0">#A Build the manager binary</span></p><p class="c7"><span class="c0">#B Copy in the go src</span></p><p class="c7"><span class="c0">#C Build</span></p><p class="c7"><span class="c0">#D Copy the controller-manager into a thin image</span></p><p class="c7"><span class="c0">In fact OKD, in the effort to distribute Kubernetes as an enterprise ready container management system, took into consideration the developer and system administrator role and built an ecosystem around it by including many different tools needed by these users. Figure 1.6 shows the OKD dashboard with two available views: Administrator and Developer. You can see that OKD dashboard includes many more options and functions and is built around both the developer and the system administrator.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 369.33px;"><img alt="" src="images/image7.png" style="width: 504.00px; height: 369.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.26in1rg"><span class="c12">Figure 1.6 OKD Web Portal lets both administrators and developers manage aspects of their OKDcluster.</span></h6><p class="c1"><span class="c0"></span></p><h3 class="c11" id="h.lnxbz9"><span class="c31">Master and Worker Nodes</span></h3><p class="c7"><span>As we mentioned earlier, Kubernetes manages multiple host machines. These machines can be physical or virtual machines and are classified into different categories based on their functions. All machines are called nodes, machines that perform &ldquo;Control Plane&rdquo; functions are sometimes referred to as aster nodes while machines that actually run the applications in containers are referred to as worker nodes. At a minimum a single cluster managed by Kubernetes needs to have one aster node and one worker node. Typically a master node does not run application code, even though it can be configured to do so. You can think of the &ldquo;Control Plane&rdquo; as the manager of the cluster. &nbsp;It takes care of many cluster functions such as scheduling, api server, data store and controllers. The diagram below depicts the different nodes and functions in Kubernetes. The worker nodes also run Kubernetes components that are essential to run &ldquo;Pods</span><span class="c0">&nbsp;(groups of containers running application code) and communicate with the control plane. One essential component that we discussed earlier is the container runtime component that is essential for managing containers. All these components are shown in figure 1.7.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 474.67px;"><img alt="" src="images/image6.png" style="width: 504.00px; height: 474.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.35nkun2"><span class="c12">Figure 1.7 Kubernetes high level architecture. Control Plane nodes called master nodes manage the cluster while worker nodes are responsible for running applications in containers.</span></h6><p class="c1"><span class="c0"></span></p><h3 class="c11" id="h.1ksv4uv"><span class="c31">Kubernetes API</span></h3><p class="c7"><span>The components we will be describing - &nbsp;Pods, Services, Ingress, etc. are all represented by resource objects inside Kubernetes. These resource objects can be created, changed or deleted by users by using the Kubernetes API. Think of the operating system we described earlier as having kernel space and user space. n Kubernetes we have a </span><span class="c0">ontrol plane&rdquo; similar to the kernel space that provides API to create objects. These objects are created in the user space in the worker nodes but managed by the control plane.</span></p><p class="c7"><span class="c0">There are tools available for users that provide a simplified access to the Kubernetes API such as &ldquo;kubectl&rdquo; in Kubernetes and &ldquo;oc&rdquo; in OKD. Users will have to write a yaml file that describes the object and pass it to the &ldquo;kubectl&rdquo; or &ldquo;oc&rdquo; command. The information is passed in &ldquo;json&rdquo; format to the Kubernetes API and executed upon by Kubernetes. We show a simple example in figure 1.8 below of creating a &ldquo;Pod&rdquo; object. The user will issue a &ldquo;kubectl&rdquo; command to create a pod, that calls the API handled by the kube-apiserver. Kube-apiserver saves information in &ldquo;etcd&rdquo; data store and consults kube-scheduler to assign a node to create the pod on. Once that is determined, kubelet on that node is tasked to ask the container runtime to create the container and run it in a pod. &lsquo;</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 509.33px;"><img alt="" src="images/image9.png" style="width: 504.00px; height: 509.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.44sinio"><span class="c12">Figure 1.8 Kubernetes creating a Pod</span></h6><p class="c1"><span class="c0"></span></p><h3 class="c11" id="h.2jxsxqh"><span class="c31">Pods, Deployments and ReplicaSets</span></h3><p class="c7"><span class="c0">There are many different concepts to describe as part of Kubernetes, however we will focus on the concepts that are relevant to running an AI/ML workload on a cluster. &nbsp;</span></p><p class="c7"><span class="c0">When using Kubeflow to run AI/ML workloads on Kubernetes it is important to understand the following concepts or components: Pods, Deployments and ReplicaSets. A Pod is the smallest deployable unit on Kubernetes. A Pod consists of one or multiple containers running on a worker node. When you deploy your AI/ML application in Kubernetes, you deploy it as a Pod. You might be wondering why we would need multiple containers to run our application? There could be many reasons and it all depends on how you design your application. A very popular example is to have one container acting as a server constantly listening on a port, and another container, called a sidecar, &nbsp;that updates the files served by the server. You will see this concept in most models serving Pods where one container handles all the REST calls to the model, and the other container handles the model code. &nbsp;The containers running in a Pod share storage and network resources and can be tightly coupled. </span></p><p class="c7"><span class="c0">Notice that we always use the word &ldquo;deploy&rdquo; when indicating running a &ldquo;Pod&rdquo; on a cluster. &nbsp;That is because running a &ldquo;Pod&rdquo; on a cluster is controlled by a resource called &ldquo;Deployment&rdquo; that the user specifies. A &ldquo;Deployment&rdquo; also controls the &ldquo;ReplicaSet&rdquo; which indicates how many identical pods the user wants to run on the cluster. An example &ldquo;Deployment&rdquo; file is described below in figure 1.9 along with a &ldquo;ReplicaSet&rdquo; indicating the desire to have three identical &ldquo;Pods&rdquo; running on the cluster at all times. Kubernetes reads this deployment file and constantly tries to match the desired state with the actual state, and keeps them matched. You will notice this concept evident when you try to delete a &ldquo;Pod&rdquo; and it keeps getting deployed again. If you want to delete a &ldquo;Pod&rdquo; permanently you will need to delete its associated &ldquo;Deployment&rdquo;. </span></p><p class="c1"><span class="c0"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 478.67px;"><img alt="" src="images/image8.png" style="width: 504.00px; height: 478.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.z337ya"><span class="c12">Figure 1.9 YAML Specification for creating a Deployment resource.</span></h6><h3 class="c11 c48" id="h.3j2qqm3"><span class="c31"></span></h3><h3 class="c11" id="h.1y810tw"><span class="c31">Services, Ingress/Routes, Istio</span></h3><p class="c11 c18"><span class="c0"></span></p><p class="c11"><span class="c0">To understand Services in Kubernetes, think of installing a web server, assigning a global Domain name to it, and now it becomes reachable globally from any machine connected to the internet. Services in Kubernetes perform a similar function, they allow users to define one or multiple pods offering the same service and Kubernetes takes care of assigning IP addresses and internal single domain names (your-svc.your-namespace.svc.cluster.local) so traffic can be load balanced between these pods. &nbsp;When serving an AI/ML model you can create multiple pods with the model and REST interface then create a Kubernetes Service to load balance traffic between the pods and get a single domain name making your model accessible. </span></p><p class="c11"><span class="c0">The domain name assigned to services and described above is accessible internally only within a cluster or namespace. This brings us to the next concept in Kubernetes called Ingress or Routes in OKD. An Ingress or a Route exposes a Service to the outside so your served model is reachable by clients outside the cluster, as shown in figure 1.10. Services may be assigned an external Domain Name that is unique and globally reachable.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 313.33px;"><img alt="" src="images/image12.png" style="width: 504.00px; height: 313.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c11"><span>&nbsp;</span><span class="c12">Figure 1.10 External traffic to Pods using Services and Ingress</span></p><p class="c11 c18"><span class="c12"></span></p><p class="c11"><span>However, a service mesh platform called </span><span>&ldquo;i</span><span>stio&rdquo; was introduced to provide multiple functions between microservices including routing that was provided by Ingress/Route. Kubeflow by default installs </span><span>stio and uses this platform to manage its microservices. In addition to routing, Istio also provides functions for specifying security rules and monitoring capabilities between microservices. You will notice, and as we will describe later, when launching the Kubeflow portal you will actually need to find the istio-system namespace or project and find the route to the &ldquo;ingress gateway&rdquo; which is part of the &ldquo;</span><span>i</span><span>stio&rdquo; installation. Figure 1.11 below shows the different new components involved in passing external traffic to pods in Kubeflow. A gateway such as the Kubeflow Gateway and a Virtual Service such as Virtual Service A are Istio components that provide routing functionality to the different pods. For detailed information on istio please visit </span><span class="c39"><a class="c26" href="https://www.google.com/url?q=http://istio.io&amp;sa=D&amp;source=editors&amp;ust=1677450530352813&amp;usg=AOvVaw0CwKHBKSPPy0HyNt65thkR">istio.io</a></span><span class="c0">&nbsp;or Kubeflow Istio (http://mng.bz/9Kll) and see figure 1.11.</span></p><p class="c11"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 481.33px;"><img alt="" src="images/image10.png" style="width: 504.00px; height: 481.33px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.4i7ojhp"><span class="c12">Figure 1.11 External traffic to Pods using Istio</span></h6><p class="c11 c18"><span class="c0"></span></p><h2 class="c11 c38" id="h.2xcytpi"><span class="c15"></span></h2><h3 class="c11" id="h.1ci93xb"><span class="c31">Storage and Networking</span></h3><p class="c7"><span class="c0">At this point we have discussed containers, deploying them in &ldquo;Pods&rdquo;, assigning a &ldquo;Service&rdquo; to them and exposing an ingress or route to them. A container running inside a &ldquo;Pod&rdquo; does not have permanent disk storage. Any files saved in the filesystem of a container are lost on restart of the container. That&#39;s why Kubernetes provides the ability to create and use permanent &nbsp;storage called &ldquo;Volumes&rdquo; or &ldquo;Persistent Volumes&rdquo;. &ldquo;Volumes&rdquo; can be described in the context of a container as shown in the example below. &ldquo;Persistent Volume&rdquo; is a resource that can be created outside of a pod definition by defining a yaml description as seen in the example below. Once a &ldquo;Persistent Volume&rdquo; is created by Kubernetes, containers can use it by creating a &ldquo;Persistent Volume Claim&rdquo; as seen in the example below. The &ldquo;Pod&rdquo; example below tells Kubernetes that this &ldquo;Persistent Volume&rdquo; will be used by this specific container. </span></p><p class="c4"><span class="c6">apiVersion</span><span class="c9">: v1</span></p><p class="c4"><span class="c6">kind</span><span class="c9">: Pod</span></p><p class="c4"><span class="c6">metadata</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">name</span><span class="c9">: appa</span></p><p class="c4"><span class="c6">spec</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">containers</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; - </span><span class="c6">image</span><span class="c9">: nakfour/pythonapp:0.1</span></p><p class="c4"><span class="c16">&nbsp; &nbsp; </span><span class="c6">name</span><span class="c9">: appa</span></p><p class="c4"><span class="c16">&nbsp; &nbsp; </span><span class="c6">volumeMounts</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; &nbsp; - </span><span class="c6">mountPath</span><span class="c9">: /data</span></p><p class="c4"><span class="c16">&nbsp; &nbsp; &nbsp; </span><span class="c6">name</span><span class="c9">: datavolume</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">volumes</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; - </span><span class="c6">name</span><span class="c9">: datavolume</span></p><p class="c3"><span class="c16">&nbsp; &nbsp; </span><span class="c6">emptyDir</span><span class="c9">: {}</span></p><p class="c3 c18"><span class="c25"></span></p><p class="c3 c18"><span class="c25"></span></p><p class="c4"><span class="c6">apiVersion</span><span class="c9">: v1</span></p><p class="c4"><span class="c6">kind</span><span class="c9">: PersistentVolume</span></p><p class="c4"><span class="c6">metadata</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">name</span><span class="c9">: example-volume</span></p><p class="c4"><span class="c6">spec</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">storageClassName</span><span class="c9">: manual</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">capacity</span><span class="c9">:</span></p><p class="c4"><span class="c16">&nbsp; &nbsp; </span><span class="c6">storage</span><span class="c9">: 1Gi</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">accessModes</span><span class="c9">:</span></p><p class="c4"><span class="c9">&nbsp; &nbsp; - ReadWriteOnce</span></p><p class="c4"><span class="c16">&nbsp; </span><span class="c6">hostPath</span><span class="c9">:</span></p><p class="c3"><span class="c16">&nbsp; &nbsp; </span><span class="c6">path</span><span class="c9">: &quot;/mnt/data&quot;</span></p><p class="c1"><span class="c40"></span></p><p class="c7"><span class="c6">apiVersion: </span><span class="c9">v1</span></p><p class="c7"><span class="c6">kind: </span><span class="c9">PersistentVolumeClaim</span></p><p class="c7"><span class="c14 c6">metadata:</span></p><p class="c7"><span class="c6">&nbsp; name: </span><span class="c9">example-pvc</span></p><p class="c7"><span class="c14 c6">spec:</span></p><p class="c7"><span class="c6">&nbsp; storageClassName: </span><span class="c9">manual</span></p><p class="c7"><span class="c14 c6">&nbsp; accessModes:</span></p><p class="c7"><span class="c14 c6">&nbsp; &nbsp; - ReadWriteOnce</span></p><p class="c7"><span class="c14 c6">&nbsp; resources:</span></p><p class="c7"><span class="c14 c6">&nbsp; &nbsp; requests:</span></p><p class="c42"><span class="c6">&nbsp; &nbsp; &nbsp; storage: </span><span class="c9">1Gi</span></p><p class="c1"><span class="c40"></span></p><p class="c17"><span class="c6">apiVersion: </span><span class="c9">v1</span></p><p class="c17"><span class="c6">kind: </span><span class="c9">Pod</span></p><p class="c17"><span class="c6 c14">metadata:</span></p><p class="c17"><span class="c6">&nbsp; name: </span><span class="c9">appa</span></p><p class="c17"><span class="c14 c6">spec:</span></p><p class="c17"><span class="c14 c6">&nbsp; volumes:</span></p><p class="c17"><span class="c6">&nbsp; &nbsp; - name: </span><span class="c9">pod-storage</span></p><p class="c17"><span class="c14 c6">&nbsp; &nbsp; &nbsp; persistentVolumeClaim:</span></p><p class="c17"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; claimName: </span><span class="c9">example-pvc</span></p><p class="c17"><span class="c14 c6">&nbsp; containers:</span></p><p class="c17"><span class="c6">&nbsp; &nbsp; - image: </span><span class="c9">nakfour/pythonapp:0.1</span></p><p class="c17"><span class="c6">&nbsp; &nbsp; &nbsp; name: </span><span class="c9">appa</span></p><p class="c17"><span class="c14 c6">&nbsp; &nbsp; &nbsp; volumeMounts:</span></p><p class="c17"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; - mountPath:</span><span class="c9">&nbsp;&quot;/data&quot;</span></p><p class="c7"><span class="c6">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; name: </span><span class="c16">pod-storage</span></p><h3 class="c11" id="h.3whwml4"><span class="c31">Operators</span></h3><p class="c7"><span>As we described earlier, Pods, Services, Deployments are all Kubernetes resource objects that can be created, deleted and modified using the Kubernetes API. To make the platform even more powerful, Kubernetes allows developers to extend this API and define new or custom resources and write application controllers that allow users to create, delete or modify these new resources. These new applications and their custom resources are called Operators. Kubernetes and OKD also provided a marketplace where developers can publish their operators for users to download and install on their clusters. Developers can publish their Operators on operatorhub.io or in O</span><span class="c10">peratorHub via the OKD dashboard shown in figure 1.12 below </span><span class="c0">. </span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 504.00px; height: 286.67px;"><img alt="" src="images/image11.png" style="width: 504.00px; height: 286.67px; margin-left: -0.00px; margin-top: -0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h6 class="c2" id="h.2bn6wsx"><span class="c12">Figure 1.12 OperatorHub on OKD dashboard</span></h6><p class="c1"><span class="c0"></span></p><p class="c7"><span class="c0">Why is this powerful and how does it relate to AI/ML workloads? As a data scientist, you will notice that almost all applications that you need to run your AI/ML workloads are provided as an Operator for you to download and install. This includes Kubeflow and Open Data Hub. Operators can run cluster wide or within a namespace or project on the cluster. When installing an application as an Operator you will notice that it will most likely install a &ldquo;Pod&rdquo; that runs the controller part of the application and a Custom Resource Definition which extends the Kubernetes API for you to create, delete or modify the resource. A good example is the &ldquo;Seldon&rdquo; operator, that will install a seldon-controller-manager pod and provide a custom resource called &ldquo;SeldonDeployment&rdquo;. When you need to deploy your model, you create a SeldonDeployment that specifies certain fields. We will explain this in more detail at a later stage.</span></p><p class="c7"><span class="c0">Operators are also powerful because they provide application management features such as auto-upgrade and metrics collection for efficient resource management and seamless application management. You can think of Operators as applications on your mobile phone that can be downloaded from the Application Store with seamless application management and upgrades. </span></p><h1 class="c44 c42 c46" id="h.qsh70q"><span class="c10 c37">Summary</span></h1><ul class="c43 lst-kix_list_1-0"><li class="c13 li-bullet-0"><span>The data science workflow consists of certain core components like data collection, data exploration and clearing, modeling and deployment to production. Kubernetes and Kubeflow provide an end-to-end tooling ecosystem for implementing these components.</span></li><li class="c17 c47 li-bullet-0"><span>Containers are light-weight software artifacts that are ideal for quick experimentation as well as scalable serving of models. Kubernetes is a container orchestration system that is at the foundations of Kubeflow. OKD is an enterprise-ready distribution of Kubernetes.</span></li><li class="c53 c47 li-bullet-0"><span>Kubeflow builds on Kubernetes and provides tools for building data science workflows. Open Data Hub adds to this tooling and runs on top of OKD.</span></li></ul><div><p class="c20"><span class="c9">&copy;Manning Publications Co. We welcome reader comments about anything in the manuscript - other than typos and </span></p><p class="c20"><span class="c9">other simple mistakes. These will be cleaned up during production of the book by copyeditors and proofreaders.</span></p><p class="c20"><span class="c16 c39"><a class="c26" href="https://www.google.com/url?q=https://livebook.manning.com/%23!/book/book-title/discussion&amp;sa=D&amp;source=editors&amp;ust=1677450530361079&amp;usg=AOvVaw07UgkQIRDHL-JA4TILYR0L">https://livebook.manning.com/#!/book/book-title/discussion</a></span><span class="c9">&nbsp;</span></p></div><hr class="c29"><div><p class="c3"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c32">&nbsp;</span><span class="c0">https://www.linuxfoundation.org/2017-linux-kernel-report-landing-page/</span></p></div><div class="c23"><p class="c3"><a href="#cmnt_ref1" id="cmnt1">[a]</a><span class="c14 c19">Proofreading Ch 1: suggestions are listed below in comments</span></p></div><div class="c23"><p class="c3"><a href="#cmnt_ref2" id="cmnt2">[b]</a><span class="c14 c19">should we delete this footer?</span></p></div><div class="c23"><p class="c3"><a href="#cmnt_ref3" id="cmnt3">[c]</a><span class="c14 c19">Question: do we describe these later? can remove the sentence otherwise</span></p></div><div class="c23"><p class="c3"><a href="#cmnt_ref4" id="cmnt4">[d]</a><span class="c14 c19">Question: not sure if we do this at some point later. Can remove if necessary</span></p></div><div class="c23"><p class="c3"><a href="#cmnt_ref5" id="cmnt5">[e]</a><span class="c14 c19">Should we delete Open Data Hub content?</span></p></div></body></html>